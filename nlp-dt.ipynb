{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP_DT\n\nI am using this notebook as a guide: https://www.kaggle.com/donmarch14/disaster-tweets-prediction-nlp-guide.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nimport string\nimport emoji\nimport re\nfrom wordcloud import WordCloud\nimport nltk #For Stemming, NLTK is needed\nfrom nltk.stem.snowball import SnowballStemmer\nimport spacy\nnlp = spacy.load('en_core_web_lg')\n\n#SKlearn\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\n# TensorFlow\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras import backend as K\nfrom tensorflow.keras.layers import Dense, Input\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\nimport matplotlib.pyplot as plt # matplotlib and seaborn for plotting\nimport seaborn as sns\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inspection (EDA)","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and test data shape\n\nprint(\"Train dataset shape : \",train_df.shape)\nprint(\"Test dataset shape : \",test_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at the number of each type of target\n\nsns.barplot(train_df['target'].value_counts().index,train_df['target'].value_counts(),palette='rocket')\nplt.title('Targets')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at the different types of keywords\n\nsns.barplot(y=train_df['keyword'].value_counts()[:25].index,x=train_df['keyword'].value_counts()[:25], orient='horizontal', palette='viridis')\nplt.title('Keywords')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split thte train data into the disaster and non-disaster tweets\n\ndisaster_tweets = train_df[train_df['target']==1]['text']\nnon_disaster_tweets = train_df[train_df['target']==0]['text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a wordcloud for each type of tweet\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Clean function removes any characters that might skew the models\n\ndef removeStopwords(text):\n    doc = nlp(text)\n    clean_text = ' '\n    for txt in doc:\n        if (txt.is_stop == False):\n            clean_text = clean_text + \" \" + str(txt)        \n    \n    return clean_text\n\ndef removePunctuations(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\ndef removeEmojis(text):\n    allchars = [c for c in text]\n    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI[\"en\"]]\n    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n    return clean_text\n\ndef removeNumbers(text):\n    clean_text = re.sub(r'\\d+', '', text)\n    return clean_text\n\ndef removeLinks(text):\n    clean_text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    #https? will match both http and https\n    #A|B, where A and B can be arbitrary REs, creates a regular expression that will match either A or B.\n    #\\S Matches any character which is not a whitespace character.\n    #+ Causes the resulting RE to match 1 or more repetitions of the preceding RE. ab+ will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’.\n    return clean_text\n\ndef clean(text):\n    text = text.lower() \n    text = removeStopwords(text)\n    text = removePunctuations(text)\n    text = removeEmojis(text)\n    text = removeNumbers(text)\n    text = removeLinks(text)\n    return text\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean the train and test data: \ntrain_df['text']=train_df.text.apply(clean)\ntest_df['text']=test_df.text.apply(clean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a wordcloud for the cleaned train data\n\ntweets = train_df['text']\nfig, ax1, = plt.subplots(1,  figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(tweets))\nax1.imshow(wordcloud1)\nax1.axis('on')\nax1.set_title('Tweets',fontsize=40);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/rftexas/text-only-bert-keras?scriptVersionId=31186559 Some data is wrong. For example, target of the training dataset at $328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226$ are given as 1 whereas they are obviously 0, since they are not related to disaster.\n\nWe change it to 0.","metadata":{}},{"cell_type":"code","source":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain_df.at[train_df['id'].isin(ids_with_target_error),'target'] = 0\ntrain_df[train_df['id'].isin(ids_with_target_error)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text normalisation\n\nLets convert all the abbreviations to its full form. Thanks to https://www.kaggle.com/rftexas/text-only-bert-keras?scriptVersionId=31186559.","metadata":{}},{"cell_type":"code","source":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"€\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converts abbreviations to their full text\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['text']=train_df.text.apply(convert_abbrev)\ntest_df['text']=test_df.text.apply(convert_abbrev)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stemming","metadata":{}},{"cell_type":"code","source":"# Finds the stem of a word\nstemmer = SnowballStemmer(language='english')\n\ntokens = train_df['text'][1].split()\nclean_text = ' '\n\nfor token in tokens:\n    print(token + ' --> ' + stemmer.stem(token))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replaces each word with its stem\n\ndef stemWord(text):\n    stemmer = SnowballStemmer(language='english')\n    tokens = text.split()\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + stemmer.stem(token)      \n    \n    return clean_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['text']=train_df.text.apply(stemWord)\ntest_df['text']=test_df.text.apply(stemWord)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lemmatisation","metadata":{}},{"cell_type":"code","source":"# Outputs the word associated with the root\n\ndef lemmatizeWord(text):\n    tokens=nlp(text)\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + token.lemma_      \n    \n    return clean_text\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['text']=train_df.text.apply(lemmatizeWord)\ntest_df['text']=test_df.text.apply(lemmatizeWord)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transforming tokens to a vector","metadata":{}},{"cell_type":"markdown","source":"## CountVectorizer","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ntrain_bag = count_vectorizer.fit_transform(train_df['text'])\ntest_bag = count_vectorizer.transform(test_df[\"text\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF\nTF-IDF (Term frequency-Inverse document frequency) Where the term frequency is the number of appearances of a term t / number of terms in the document. The inverse document frequency is a score of how rare a given word is : IDF=$1+log(\\tfrac{N}{n})$. Where $N$ is the number of documents and $n$ is the number of documents with a term t. ","metadata":{}},{"cell_type":"code","source":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train_df['text'])\ntest_tfidf = tfidf.transform(test_df[\"text\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Vectors / Word embeddings","metadata":{}},{"cell_type":"code","source":"with nlp.disable_pipes():\n    train_vectors = np.array([nlp(text).vector for text in train_df.text])\n    test_vectors = np.array([nlp(text).vector for text in test_df.text])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a Text Classification Model","metadata":{}},{"cell_type":"markdown","source":"## Support Vector Machines","metadata":{}},{"cell_type":"code","source":"# Set dual=False to speed up training, and it's not needed\n\nsvc_wordEmbed = LinearSVC(random_state=42, dual=False, max_iter=10000)\nsvc_wordEmbed.fit(train_vectors, train_df.target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate using the F1 score\n\nscores = model_selection.cross_val_score(svc_wordEmbed, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"xgb_wordEmbed = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(xgb_wordEmbed, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"code","source":"clf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_bag, train_df[\"target\"], cv=3, scoring=\"f1\")\nclf_NB.fit(train_bag, train_df[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train_df[\"target\"], cv=3, scoring=\"f1\")\nclf_NB_TFIDF.fit(train_tfidf, train_df[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"clf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_bag, train_df[\"target\"], cv=3, scoring=\"f1\")\nclf.fit(train_bag, train_df[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nclf_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf_tfidf, train_tfidf, train_df[\"target\"], cv=3, scoring=\"f1\")\nclf_tfidf.fit(train_bag, train_df[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network","metadata":{}},{"cell_type":"code","source":"train_vectors.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions for evaulating the NN\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Learning rate\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_f1_m', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\n\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=5, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Neural Network\n\nnn = keras.Sequential([\n    layers.Dense(256, activation='relu', input_shape=[7613,300]),\n    layers.Dropout(0.4),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.4),\n    layers.Dense(1,activation='sigmoid')\n])\n\nnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=[f1_m])\nhistory=nn.fit(\n    train_vectors,train_df[\"target\"],\n    validation_split=0.1,\n    batch_size=128,\n    epochs=25,\n    callbacks=[early_stopping,learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['f1_m','val_f1_m']].plot()\nhistory_frame.loc[:, ['loss','val_loss']].plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predictions from the NN\npred = nn.predict(test_vectors)\n\npred[pred > 0.5] = 1\npred[pred <= 0.5] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT\n\nBERT (Bidirectional Encoder Representations from Transformers), unlike conventional text classifiers BERT does not read the text from left to right. Instead BERT is non-directional meaning it reads the whole string before making any judgements.","metadata":{}},{"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n # get the official tokenization created by the Google team","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tokenization\n\n# Helper function for BERT\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Builds a BERT model\n\ndef build_model(bert_layer, max_len = 128, lr = 1e-5):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"segment_ids\")\n        \n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    dense_out = Dense(1,activation=\"relu\")(pooled_output)\n    drop_out = tf.keras.layers.Dropout(0.8)(dense_out)\n    out = Dense(1,activation=\"sigmoid\")(pooled_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    adam = tf.keras.optimizers.Adam(lr)\n    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=[f1_m])\n        \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making the Submission","metadata":{}},{"cell_type":"code","source":"ss = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nss.info()\nss.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = nn.predict(test_vectors)\ntarget = []\nfor p in pred:\n    if p > 0.5:\n        target.append(1)\n    else:\n        target.append(0)\n\nss['target'] = target\nss.to_csv('nlpdt_nn.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}