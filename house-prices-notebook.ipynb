{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# House Prices Notebook\n\nThis notebook is an entry for the House Prices Advanced Regression competition. I tried this once and got a score of 0.7 (not very good), I then went and looked at some notebooks and noticed a lot of things that I did wrong first time round. Here is my updated attempt. One notebook I found particularly was this one by Pedro Marcelino: https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python#COMPREHENSIVE-DATA-EXPLORATION-WITH-PYTHON. And this introduction to linear models: https://www.kaggle.com/omercansvgn/machine-learning-tutorial-for-beginners.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # Data processing\nimport matplotlib.pyplot as plt # Visualisation\nimport seaborn as sns # Visualisation\nfrom scipy import stats # Stats\nfrom scipy.stats import norm # Normalising the data\nfrom sklearn.preprocessing import StandardScaler # Preprocessing\nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inspecting the Data\n\nFirst we need to look at the data to see what variables we are going to use in our model","metadata":{}},{"cell_type":"code","source":"# Train Data\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n# Test Data\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntrain.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sale Price\ntrain['SalePrice'].describe()\nplt.style.use('ggplot')\nsns.distplot(train['SalePrice'], color='blue')\nplt.xticks(rotation=45)\nplt.ylabel('Frequency')\nplt.title('Sale Price Histogram')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sale price has a positive skewness. This will need to be adjusted for later.","metadata":{}},{"cell_type":"code","source":"# Heatmap to find the variables that are correlated most with the Sale Price\ncorrmat = train.corr()\ncols = corrmat.nlargest(10, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1)\nhm = sns.heatmap(cm, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at this heatmap here is my assessment of the variables:\n* OverallQual is the most correlated so I will use this\n* GrLivArea is also highly correlated so I will use this\n* GarageCars and GarageArea are similar variables, so I will only keep GarageCars\n* Similarly TotalBsmtSF and 1stFlrSF are similar variavles, so I will only keep TotalBsmtSF\n* FullBath is a bit wierd that this is correlated but I'll use it\n* TotalRmsAbvGrd is very similar to GrLivArea so I won't use it\n* YearBuilt is only slightly correlated ","metadata":{}},{"cell_type":"code","source":"# Creating the training data\ncolumns = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\ntrain_data = train[columns]\n# Scatter Plot of Variables\nsns.set()\nsns.pairplot(train_data, size = 2.5)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing data\ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent = (train_data.isnull().sum()/train_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's handy, there isn't any missing data. I was going to create a separate heading for cleaning the data. But we don't need to.","metadata":{}},{"cell_type":"markdown","source":"# EDA\n\nNow it's time to look at how our variables are related to the sale price. ","metadata":{}},{"cell_type":"code","source":"# OverallQual\nsns.boxplot(data=train_data, x='OverallQual', y='SalePrice', palette=sns.color_palette(), linewidth=1)\nplt.title('OverallQual BoxPlot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's not known how the overall quality was measured, and also the growth is non-linear (maybe polynomial or exponential, impossible to say).","metadata":{}},{"cell_type":"code","source":"# GrLivArea\nplt.scatter(x=train_data['GrLivArea'], y=train_data['SalePrice'], alpha=0.4)\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')\nplt.title('GrLivArea ScatterPlot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like there are two outliers in the bottom right corner. Let's remove them ","metadata":{}},{"cell_type":"code","source":"# Removing the points\ntrain.sort_values(by = 'GrLivArea', ascending = False)[:2]\ntrain = train.drop(train[train['Id'] == 1299].index)\ntrain = train.drop(train[train['Id'] == 524].index)\ntrain_data = train[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GarageCars\nsns.boxplot(data=train_data, x='GarageCars', y='SalePrice', palette=sns.color_palette(), linewidth=1)\nplt.title('GarageCars BoxPlot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TotalBsmtSF\nplt.scatter(x=train_data['TotalBsmtSF'], y=train_data['SalePrice'], alpha=0.4)\nplt.xlabel('TotalBsmtSF')\nplt.ylabel('SalePrice')\nplt.title('TotalBsmtSF ScatterPlot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two things to note from this scatter plot. First there is a large cluster of points with no basement size, which may cause problems. Secondly the point with the highest TotalBsmtSF (with a sale price just below 300,000) is an outlier and hence I will remove it.","metadata":{}},{"cell_type":"code","source":"# Removing the point\ntrain.sort_values(by = 'TotalBsmtSF', ascending = False)[:1]\ntrain = train.drop(train[train['Id'] == 333].index)\ntrain_data = train[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FullBath\nsns.boxplot(data=train_data, x='FullBath', y='SalePrice', palette=sns.color_palette(), linewidth=1)\nplt.title('FullBath BoxPlot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# YearBuilt\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(data=train_data, x='YearBuilt', y='SalePrice', linewidth=1)\nplt.xticks(rotation=90)\nplt.title('YearBuilt BoxPlot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# YearBuilt\nf, ax = plt.subplots(figsize=(16, 8))\nplt.scatter(x=train_data['YearBuilt'], y=train_data['SalePrice'], alpha=0.4)\nplt.xlabel('YearBuilt')\nplt.ylabel('SalePrice')\nplt.xticks(rotation=90)\nplt.title('YearBuilt ScatterPlot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like this graph has an exponential growth rate. Kind of sucks for people who might be looking to buy a house in the near future like myself. Anyway, I now want to fix the skewed saleprice data so that it can be better used for our model.","metadata":{}},{"cell_type":"code","source":"# Histogram and normal plot\nsns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Handily taking the log of the y value is a neat shortcut for fixing positive skewness which works almost all the time.","metadata":{}},{"cell_type":"code","source":"# Reshaping the SalePrice data\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\ntrain_data['SalePrice'] = train['SalePrice']\n\n# The original graphs but with the reshaped sale prices\nsns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Much better! Let's look at the other numeric data.","metadata":{}},{"cell_type":"code","source":"# GrLivArea\nkol = 'GrLivArea'\n# Reshaping the data\ntrain[kol] = np.log1p(train[kol])\ntrain_data[kol] = train[kol]\n\n# Graphs\nsns.distplot(train[kol], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train[kol], plot=plt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['TotalBsmtSF'] = np.log1p(train['TotalBsmtSF'])\ntrain_data['TotalBsmtSF'] = train['TotalBsmtSF']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Graphs for TotalBsmtSF\nsns.distplot(train_data['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train_data['TotalBsmtSF'], plot=plt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Splitting up that data\ndata = train_data.drop(['SalePrice'], axis=1)\nlabels = train_data['SalePrice']\nx_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.3,random_state=42)\n# Model\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\n\nmse = mean_squared_error(y_test, model.predict(x_test)) \nrmse = np.sqrt(mse) \n# Results\nprint('Score:',model.score(x_test, y_test))\nprint('Model Intercept:',model.intercept_)\nprint('Model Coef:',model.coef_)\nprint('RMSE:',rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(x_test)\nsns.distplot(pred, fit=norm);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fixing the Test Data\n\nThe test data also needs to be transformed so that our model can use it to make predictions.","metadata":{}},{"cell_type":"code","source":"cols = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\ntest_data = test[cols]\ntest_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for null values\ntotal = test_data.isnull().sum().sort_values(ascending=False)\npercent = (test_data.isnull().sum()/test_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will assume that these values are 0\ntest_data['GarageCars'] = test_data['GarageCars'].fillna(0)\ntest_data['TotalBsmtSF'] = test_data['TotalBsmtSF'].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now I need to transform the test data\ntest_data['GrLivArea'] = np.log1p(test_data['GrLivArea'])\ntest_data['TotalBsmtSF'] = np.log1p(test_data['TotalBsmtSF'])\ntest_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions\nlog_prediction = model.predict(test_data)\nsns.distplot(log_prediction, fit=norm);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the Submission df\ntest['SalePrice'] = np.exp(log_prediction)\nsaleprice = test['SalePrice'] - 1\ndf_submit = pd.DataFrame({'Id': test['Id'], 'SalePrice': saleprice})\ndf_submit.to_csv('Submit.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}